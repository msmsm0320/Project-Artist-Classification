{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovxAEsNIa4B4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.style as style\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Colab\\ Notebooks\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(df, df['artist'].values, test_size=0.2)\n",
        "print(\"Number of posters for training: \", len(X_train))\n",
        "print(\"Number of posters for validation: \", len(X_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7aJCc0yea6j",
        "outputId": "8000c185-4988-4e0c-d77f-31e0c9ef4e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks'\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "Number of posters for training:  4728\n",
            "Number of posters for validation:  1183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GoogleNet 모델 로드\n",
        "base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(244,244,3))\n",
        "\n",
        "# 새로운 Fully Connected Layer 추가\n",
        "x= base_model.output\n",
        "x= Flatten()(x)\n",
        "predictions = Dense(50, activation = 'softmax')(x)\n",
        "\n",
        "# 전체 모델 구성\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 기존 모델 레이어 동결\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss= 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# 콜백함수 설정 \n",
        "CP = ModelCheckpoint(filepath='model/' +\n",
        "                     'InceptionV3-Sigmoid-{epoch:03d}-{loss:.4f}-{val_loss:.4f}.hdf5',\n",
        "     monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=5, verbose=1, min_lr=0.00005)\n",
        "CALLBACK = [CP, LR]"
      ],
      "metadata": {
        "id": "KQFOUap7egsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATAGEN_TRAIN = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    data_format=\"channels_last\",\n",
        "    validation_split=0.10) # Train / Validation\n",
        " \n",
        "# Generator의 instance 생성 (Train)\n",
        "TRAIN_GENERATOR = DATAGEN_TRAIN.flow_from_dataframe(\n",
        "                                        dataframe = X_train, x_col='img_path', y_col='artist',\n",
        "                                        target_size=(244, 244), \n",
        "                                        class_mode='categorical',\n",
        "                                        batch_size=32, shuffle=True,\n",
        "                                        subset = \"training\")\n",
        " \n",
        "VALID_GENERATOR = DATAGEN_TRAIN.flow_from_dataframe(\n",
        "                                        dataframe = X_train, x_col='img_path', y_col='artist',\n",
        "                                        target_size=(244, 244), \n",
        "                                        class_mode='categorical',\n",
        "                                        batch_size=32, shuffle=True,\n",
        "                                        subset = \"validation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6s_LkEkiSqR",
        "outputId": "b4233a94-1bc9-4e37-93d0-239b98b1c084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4256 validated image filenames belonging to 50 classes.\n",
            "Found 472 validated image filenames belonging to 50 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(TRAIN_GENERATOR, epochs=20, callbacks=CALLBACK, shuffle=True, validation_data=VALID_GENERATOR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNJ4cwp1kJGX",
        "outputId": "99f57db7-2973-4933-b291-031851b80aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 26.1936 - accuracy: 0.1877\n",
            "Epoch 1: val_loss improved from inf to 14.36413, saving model to model/InceptionV3-Sigmoid-001-26.1936-14.3641.hdf5\n",
            "133/133 [==============================] - 136s 993ms/step - loss: 26.1936 - accuracy: 0.1877 - val_loss: 14.3641 - val_accuracy: 0.3093 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 15.1497 - accuracy: 0.3386\n",
            "Epoch 2: val_loss improved from 14.36413 to 13.03530, saving model to model/InceptionV3-Sigmoid-002-15.1497-13.0353.hdf5\n",
            "133/133 [==============================] - 130s 975ms/step - loss: 15.1497 - accuracy: 0.3386 - val_loss: 13.0353 - val_accuracy: 0.3708 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 14.7301 - accuracy: 0.3750\n",
            "Epoch 3: val_loss did not improve from 13.03530\n",
            "133/133 [==============================] - 128s 963ms/step - loss: 14.7301 - accuracy: 0.3750 - val_loss: 13.5155 - val_accuracy: 0.4131 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 15.3570 - accuracy: 0.4084\n",
            "Epoch 4: val_loss did not improve from 13.03530\n",
            "133/133 [==============================] - 128s 961ms/step - loss: 15.3570 - accuracy: 0.4084 - val_loss: 17.8232 - val_accuracy: 0.3602 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 15.5015 - accuracy: 0.4316\n",
            "Epoch 5: val_loss did not improve from 13.03530\n",
            "133/133 [==============================] - 128s 961ms/step - loss: 15.5015 - accuracy: 0.4316 - val_loss: 19.6081 - val_accuracy: 0.4343 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 14.5111 - accuracy: 0.4608\n",
            "Epoch 6: val_loss did not improve from 13.03530\n",
            "133/133 [==============================] - 128s 961ms/step - loss: 14.5111 - accuracy: 0.4608 - val_loss: 20.3907 - val_accuracy: 0.3919 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 13.6936 - accuracy: 0.4868\n",
            "Epoch 7: val_loss did not improve from 13.03530\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "133/133 [==============================] - 128s 959ms/step - loss: 13.6936 - accuracy: 0.4868 - val_loss: 18.3071 - val_accuracy: 0.4131 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 9.6275 - accuracy: 0.5832\n",
            "Epoch 8: val_loss did not improve from 13.03530\n",
            "133/133 [==============================] - 128s 961ms/step - loss: 9.6275 - accuracy: 0.5832 - val_loss: 14.9849 - val_accuracy: 0.4555 - lr: 5.0000e-04\n",
            "Epoch 9/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 8.9369 - accuracy: 0.5801\n",
            "Epoch 9: val_loss did not improve from 13.03530\n",
            "133/133 [==============================] - 128s 961ms/step - loss: 8.9369 - accuracy: 0.5801 - val_loss: 14.2350 - val_accuracy: 0.4894 - lr: 5.0000e-04\n",
            "Epoch 10/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 7.9219 - accuracy: 0.6097\n",
            "Epoch 10: val_loss did not improve from 13.03530\n",
            "133/133 [==============================] - 128s 959ms/step - loss: 7.9219 - accuracy: 0.6097 - val_loss: 15.2796 - val_accuracy: 0.4873 - lr: 5.0000e-04\n",
            "Epoch 11/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 7.5687 - accuracy: 0.6151\n",
            "Epoch 11: val_loss did not improve from 13.03530\n",
            "133/133 [==============================] - 128s 961ms/step - loss: 7.5687 - accuracy: 0.6151 - val_loss: 15.4744 - val_accuracy: 0.4364 - lr: 5.0000e-04\n",
            "Epoch 12/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 7.8739 - accuracy: 0.6104\n",
            "Epoch 12: val_loss did not improve from 13.03530\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "133/133 [==============================] - 128s 961ms/step - loss: 7.8739 - accuracy: 0.6104 - val_loss: 14.3663 - val_accuracy: 0.4640 - lr: 5.0000e-04\n",
            "Epoch 13/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 5.5063 - accuracy: 0.6769\n",
            "Epoch 13: val_loss improved from 13.03530 to 11.35477, saving model to model/InceptionV3-Sigmoid-013-5.5063-11.3548.hdf5\n",
            "133/133 [==============================] - 128s 961ms/step - loss: 5.5063 - accuracy: 0.6769 - val_loss: 11.3548 - val_accuracy: 0.5085 - lr: 2.5000e-04\n",
            "Epoch 14/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 5.2649 - accuracy: 0.6748\n",
            "Epoch 14: val_loss improved from 11.35477 to 11.26175, saving model to model/InceptionV3-Sigmoid-014-5.2649-11.2617.hdf5\n",
            "133/133 [==============================] - 127s 956ms/step - loss: 5.2649 - accuracy: 0.6748 - val_loss: 11.2617 - val_accuracy: 0.5593 - lr: 2.5000e-04\n",
            "Epoch 15/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 5.0287 - accuracy: 0.6915\n",
            "Epoch 15: val_loss did not improve from 11.26175\n",
            "133/133 [==============================] - 126s 946ms/step - loss: 5.0287 - accuracy: 0.6915 - val_loss: 11.7301 - val_accuracy: 0.5191 - lr: 2.5000e-04\n",
            "Epoch 16/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 4.8420 - accuracy: 0.6833\n",
            "Epoch 16: val_loss improved from 11.26175 to 10.61999, saving model to model/InceptionV3-Sigmoid-016-4.8420-10.6200.hdf5\n",
            "133/133 [==============================] - 127s 957ms/step - loss: 4.8420 - accuracy: 0.6833 - val_loss: 10.6200 - val_accuracy: 0.5593 - lr: 2.5000e-04\n",
            "Epoch 17/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 4.6590 - accuracy: 0.7014\n",
            "Epoch 17: val_loss did not improve from 10.61999\n",
            "133/133 [==============================] - 126s 948ms/step - loss: 4.6590 - accuracy: 0.7014 - val_loss: 12.6065 - val_accuracy: 0.5127 - lr: 2.5000e-04\n",
            "Epoch 18/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 4.5282 - accuracy: 0.7007\n",
            "Epoch 18: val_loss did not improve from 10.61999\n",
            "133/133 [==============================] - 126s 951ms/step - loss: 4.5282 - accuracy: 0.7007 - val_loss: 11.7524 - val_accuracy: 0.5064 - lr: 2.5000e-04\n",
            "Epoch 19/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 4.4123 - accuracy: 0.6971\n",
            "Epoch 19: val_loss did not improve from 10.61999\n",
            "133/133 [==============================] - 126s 949ms/step - loss: 4.4123 - accuracy: 0.6971 - val_loss: 10.9552 - val_accuracy: 0.5487 - lr: 2.5000e-04\n",
            "Epoch 20/20\n",
            "133/133 [==============================] - ETA: 0s - loss: 4.3073 - accuracy: 0.7119\n",
            "Epoch 20: val_loss did not improve from 10.61999\n",
            "133/133 [==============================] - 126s 951ms/step - loss: 4.3073 - accuracy: 0.7119 - val_loss: 12.5881 - val_accuracy: 0.5191 - lr: 2.5000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X_test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "DATAGEN_TEST = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    data_format=\"channels_last\")\n",
        "\n",
        "TEST_GENERATOR = DATAGEN_TEST.flow_from_dataframe(\n",
        "                                        dataframe=X_test, x_col='img_path', y_col='id',\n",
        "                                        target_size=(244, 244), class_mode='raw',\n",
        "                                        batch_size=32, shuffle=False)\n",
        "\n",
        "TEST_Prediction = model.predict_generator(TEST_GENERATOR, verbose=1)\n",
        "\n",
        "artist_num = np.argmax(TEST_Prediction, axis = 1)\n",
        "num_list = pd.DataFrame(artist_num, columns = ['artist'])\n",
        "num_list.to_csv(\"ansTgooglenet_split_32.csv\", index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdkGOrBfqlCc",
        "outputId": "2414e6ac-a53a-40b5-9665-be7e39bca9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12670 validated image filenames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-af06648344db>:14: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  TEST_Prediction = model.predict_generator(TEST_GENERATOR, verbose=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "396/396 [==============================] - 4096s 10s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"ansTgooglenet_split_32.csv\")\n",
        "pic = pd.read_csv(\"sample_submission.csv\")\n",
        "artist_info = pd.read_csv(\"artists_info.csv\")\n",
        "\n",
        "con = pd.concat([pic, df], axis=1)\n",
        "\n",
        "pre_ans = con.drop('artist', axis=1)\n",
        "\n",
        "num_50 = np.arange(50)\n",
        "\n",
        "new_df = pd.DataFrame(artist_info['name'])\n",
        "new_df['num'] = num_50\n",
        "\n",
        "list1 = df.to_dict()\n",
        "list2 = list1['artist'].values()\n",
        "ans_num = list(list2)\n",
        "\n",
        "dict1 = new_df['name'].to_dict()\n",
        "\n",
        "artist_name = []\n",
        "for i in ans_num:\n",
        "    artist_name.append(dict1[i])\n",
        "\n",
        "last_ans = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "last_ans.rename(columns = {'artist':'artist_name'},inplace=True)\n",
        "\n",
        "last_ans['artist'] = artist_name\n",
        "last_ans.drop('artist_name', axis=1, inplace=True)\n",
        "\n",
        "last_ans.to_csv(\"answer_googlenet_split_32.csv\", index=False)"
      ],
      "metadata": {
        "id": "1bw7lnwKtI6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkGRqImHNOG0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}